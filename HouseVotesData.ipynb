{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HouseVotesData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC6zmEj1BIzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlo6ZKH2C57w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the data\n",
        "dataframe = []\n",
        "file = open(\"House-votes-data.TXT\", \"r\")\n",
        "for line in file:\n",
        "    dataframe.append(line.rstrip())\n",
        "random.shuffle(dataframe)\n",
        "split_length = int(len(dataframe)/5)\n",
        "set1 = dataframe[:split_length]\n",
        "set2 = dataframe[split_length:split_length * 2]\n",
        "set3 = dataframe[split_length * 2:split_length * 3]\n",
        "set4 = dataframe[split_length * 3:split_length * 4]\n",
        "set5 = dataframe[split_length * 4:]\n",
        "data = [set1, set2, set3, set4, set5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlgTNMSBW5Zg",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J8hvjF6GGAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayesClassifier:\n",
        "  def __init__(self):\n",
        "    self.apriori_dem = 0\n",
        "    self.apriori_rep = 0\n",
        "    self.count_democrat = 0\n",
        "    self.count_republican = 0\n",
        "    self.democrat = {'y':[0]*16, 'n':[0]*16}\n",
        "    self.republican = {'y':[0]*16, 'n':[0]*16}\n",
        "\n",
        "  def train(self, train):\n",
        "    train_all = []\n",
        "    train_fill = []\n",
        "    for line in train:\n",
        "        if('?' in line):\n",
        "            train_fill.append(line)\n",
        "        else:\n",
        "            train_all.append(line)\n",
        "    for line in train_all:\n",
        "        attb = line.split(',')\n",
        "        last = attb[len(attb) - 1]\n",
        "        if(last == 'democrat'):\n",
        "            self.count_democrat += 1\n",
        "            for index in range(len(attb) - 1):\n",
        "                if(attb[index] == 'y'):\n",
        "                    self.democrat['y'][index] += 1\n",
        "                else:\n",
        "                    self.democrat['n'][index] += 1\n",
        "        else:\n",
        "            self.count_republican += 1\n",
        "            for index in range(len(attb) - 1):\n",
        "                if(attb[index] == 'y'):\n",
        "                    self.republican['y'][index] += 1\n",
        "                else:\n",
        "                    self.republican['n'][index] += 1\n",
        "\n",
        "    for line in train_fill:\n",
        "        attb = line.split(',')\n",
        "        last = attb[len(attb) - 1]\n",
        "        if(last == 'democrat'):\n",
        "            self.count_democrat += 2\n",
        "            for index in range(len(attb) - 1):\n",
        "                if(attb[index] == 'y'):\n",
        "                    self.democrat['y'][index] += 2\n",
        "                elif(attb[index] == 'n'):\n",
        "                    self.democrat['n'][index] += 2\n",
        "                else:\n",
        "                    self.democrat['y'][index] += 1\n",
        "                    self.democrat['n'][index] += 1\n",
        "        else:\n",
        "            self.count_republican += 2\n",
        "            for index in range(len(attb) - 1):\n",
        "                if(attb[index] == 'y'):\n",
        "                    self.republican['y'][index] += 2\n",
        "                elif(attb[index] == 'n'):\n",
        "                    self.republican['n'][index] += 2\n",
        "                else:\n",
        "                    self.republican['y'][index] += 1\n",
        "                    self.republican['n'][index] += 1  \n",
        "\n",
        "    self.apriori_dem = (self.count_democrat)/(self.count_democrat + self.count_republican)\n",
        "    self.apriori_rep = 1 - self.apriori_dem\n",
        "  \n",
        "  def predict(self, test):\n",
        "    true_pos = 0\n",
        "    true_neg = 0\n",
        "    false_pos = 0\n",
        "    false_neg = 0\n",
        "    predictions = []\n",
        "    for line in test:\n",
        "        attb = line.split(',')\n",
        "        ind = -1\n",
        "        p_dem = self.apriori_dem\n",
        "        p_rep = self.apriori_rep\n",
        "        for index in range(len(attb) - 1):\n",
        "            if(attb[index]  == 'y' or attb[index] == 'n'):\n",
        "                p_dem = p_dem * (self.democrat[attb[index]][index]/self.count_democrat)\n",
        "                p_rep = p_rep * (self.republican[attb[index]][index]/self.count_republican)\n",
        "\n",
        "            else:\n",
        "                if(self.democrat['y'][index] > self.democrat['n'][index]):\n",
        "                    p_dem = p_dem * (self.democrat['y'][index]/self.count_democrat)\n",
        "                else:\n",
        "                    p_dem = p_dem * (self.democrat['n'][index]/self.count_democrat)\n",
        "                if(self.republican['y'][index] > self.republican['n'][index]):\n",
        "                    p_rep = p_rep * (self.republican['y'][index]/self.count_republican)\n",
        "                else:\n",
        "                    p_rep = p_rep * (self.republican['n'][index]/self.count_republican)\n",
        "\n",
        "        if(p_dem > p_rep):\n",
        "          predictions.append('democrat')\n",
        "        else:\n",
        "          predictions.append('republican')\n",
        "\n",
        "    return predictions\n",
        "  \n",
        "  def metrics(self, predictions, actual_values):\n",
        "    true_pos = 0\n",
        "    true_neg = 0\n",
        "    false_pos = 0\n",
        "    false_neg = 0    \n",
        "    for i in range(len(predictions)):\n",
        "      if predictions[i] == 'democrat':\n",
        "        if actual_values[i] =='democrat':\n",
        "          true_neg += 1\n",
        "        else:\n",
        "          false_pos += 1\n",
        "      else:\n",
        "        if actual_values[i] == 'republican':\n",
        "          true_pos += 1\n",
        "        else:\n",
        "          false_neg += 1\n",
        "\n",
        "    total =   true_pos + false_pos + false_neg + true_neg\n",
        "    confMat = [[true_pos,false_pos],[false_neg,true_neg]] \n",
        "    metrics_dict = {  'accuracy' : (true_pos + true_neg)/total,\n",
        "                      'recall' : true_pos/(true_pos + false_pos),\n",
        "                      'precision' : true_pos/(true_pos + false_neg),\n",
        "                      'confMat'  :  confMat\n",
        "      \n",
        "    }\n",
        "    return metrics_dict    \n",
        "\n",
        "  def configs(self):\n",
        "    return {'apriori_dem' :self.apriori_dem ,\n",
        "    'apriori_rep':self.apriori_rep,\n",
        "    'count_democrat':self.count_democrat ,\n",
        "    'count_republican':self.count_republican ,\n",
        "    'dem':self.democrat ,\n",
        "    'rep':self.republican }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVMbZpPNEUCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K fold validation\n",
        "classifiers = []\n",
        "metrics_dict = []\n",
        "for num in range(5):\n",
        "    test = data[num]\n",
        "    train = []\n",
        "    for x in range(5):\n",
        "        if(num == x):\n",
        "            continue\n",
        "        train.append(data[x])\n",
        "    train = [item for sublist in train for item in sublist]\n",
        "\n",
        "    classifiers.append(NaiveBayesClassifier())\n",
        "    classifiers[num].train(train)\n",
        "    predictions = classifiers[num].predict(test)\n",
        "\n",
        "    actual_values = []\n",
        "    for case in test:\n",
        "      actual_values.append(case.split(',')[-1])    \n",
        "      \n",
        "    metrics_dict.append(classifiers[num].metrics(predictions, actual_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI5juPsQSyyM",
        "colab_type": "code",
        "outputId": "488a57d0-4dbc-458f-9427-655262ca1b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print(\"Metrics of each Plain Naive Bayes Classifier in 5 fold validation\")\n",
        "print(\"First row of confusion matrix corresponds to 'republican' label\")\n",
        "i = 1\n",
        "for metric in metrics_dict:\n",
        "  print(\"Model \",i,\" \",metric)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics of each Plain Naive Bayes Classifier in 5 fold validation\n",
            "First row of confusion matrix corresponds to 'republican' label\n",
            "Model  1   {'accuracy': 0.8620689655172413, 'recall': 0.8974358974358975, 'precision': 0.813953488372093, 'confMat': [[35, 4], [8, 40]]}\n",
            "Model  2   {'accuracy': 0.8620689655172413, 'recall': 0.9166666666666666, 'precision': 0.7857142857142857, 'confMat': [[33, 3], [9, 42]]}\n",
            "Model  3   {'accuracy': 0.9425287356321839, 'recall': 1.0, 'precision': 0.8571428571428571, 'confMat': [[30, 0], [5, 52]]}\n",
            "Model  4   {'accuracy': 0.9310344827586207, 'recall': 0.90625, 'precision': 0.90625, 'confMat': [[29, 3], [3, 52]]}\n",
            "Model  5   {'accuracy': 0.9310344827586207, 'recall': 0.9354838709677419, 'precision': 0.8787878787878788, 'confMat': [[29, 2], [4, 52]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0lLhSAqS4JB",
        "colab_type": "code",
        "outputId": "9d65dabc-1e6c-4724-ec74-7816372f3f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(\"5 fold validation metrics of Plain Naive Bayes Classifier\")\n",
        "acc = 0\n",
        "for lst in metrics_dict:\n",
        "    acc += lst['accuracy']\n",
        "acc = acc/5\n",
        "print(\"\\nAccuracy:\", acc)\n",
        "\n",
        "#finding recall\n",
        "rec = 0\n",
        "for lst in metrics_dict:\n",
        "    rec += lst['recall']\n",
        "rec = rec/5\n",
        "print(\"\\nRecall:\", rec)\n",
        "\n",
        "#finding precision\n",
        "prec = 0\n",
        "for lst in metrics_dict:\n",
        "    prec += lst['precision']\n",
        "prec = prec/5\n",
        "print(\"\\nPrecision:\", prec)\n",
        "\n",
        "#finding F score\n",
        "f_score = 0\n",
        "print(\"\\nF-score:\", (2*prec*rec)/(prec+rec))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 fold validation metrics of Plain Naive Bayes Classifier\n",
            "\n",
            "Accuracy: 0.9057471264367816\n",
            "\n",
            "Recall: 0.9311672870140612\n",
            "\n",
            "Precision: 0.848369702003423\n",
            "\n",
            "F-score: 0.8878423080552144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ99ByPnpw8G",
        "colab_type": "text"
      },
      "source": [
        "#Randomized Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HglkvM-Jjdaq",
        "colab": {}
      },
      "source": [
        "# Boosting \n",
        "class randomBoostClassifier:\n",
        "  def __init__(self, classifier, numberBoosting, samplingRatio):\n",
        "    self.numberBoosting = numberBoosting\n",
        "    self.samplingRatio = samplingRatio\n",
        "    self.classifiers = [classifier() for _ in range(numberBoosting)]\n",
        "    self.alphas = [0]*numberBoosting\n",
        "  \n",
        "  def train(self, train):\n",
        "    train_size = len(train)\n",
        "    data_weights = [1/train_size] * train_size\n",
        "    sample_size = int(self.samplingRatio * train_size)\n",
        "\n",
        "    target_value =[]\n",
        "    for case in train:\n",
        "      target_value.append(case.split(',')[-1])   \n",
        "    # Randomised Boosting\n",
        "    for i in range(self.numberBoosting):\n",
        "      indices = np.random.randint(train_size, size = sample_size)\n",
        "      sampled_data = {}  \n",
        "      for ind in indices:\n",
        "          sampled_data[ind] = train[ind]\n",
        "      self.classifiers[i].train(list(sampled_data.values()))\n",
        "      data_weights = [1/train_size] * train_size\n",
        "\n",
        "      # Boosting\n",
        "      error = 0\n",
        "      for j in indices: \n",
        "          predicted_value = self.classifiers[i].predict([sampled_data[j]])[0]\n",
        "          if predicted_value != target_value[j]:\n",
        "            error += data_weights[j]\n",
        "\n",
        "      # Normalising error\n",
        "      error = error / sum(data_weights)\n",
        "\n",
        "      # Setting alpha\n",
        "      if error!=0:\n",
        "        self.alphas[i] = (1/2)*math.log((1-error)/error)\n",
        "      else: \n",
        "        self.alphas[i] = 349\n",
        "\n",
        "  def predict(self, test):\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(test)):\n",
        "      all_predictions = []\n",
        "      for j in range(self.numberBoosting):\n",
        "        if self.classifiers[j].predict([test[i]])[0] == 'democrat':\n",
        "          all_predictions.append(self.alphas[j])\n",
        "        else:\n",
        "          all_predictions.append(-1* self.alphas[j])\n",
        "        \n",
        "      if sum(all_predictions) > 0:\n",
        "        predictions.append('democrat')\n",
        "      else:\n",
        "        predictions.append('republican')\n",
        "      \n",
        "    return predictions\n",
        "\n",
        "  def metrics(self, predictions, actual_values):\n",
        "      true_pos = 0\n",
        "      true_neg = 0\n",
        "      false_pos = 0\n",
        "      false_neg = 0   \n",
        "      for i in range(len(predictions)):\n",
        "        if predictions[i] == 'democrat':\n",
        "          if actual_values[i] =='democrat':\n",
        "            true_neg += 1\n",
        "          else:\n",
        "            false_pos += 1\n",
        "        else:\n",
        "          if actual_values[i] == 'republican':\n",
        "            true_pos += 1\n",
        "          else:\n",
        "            false_neg += 1\n",
        "      total =   true_pos + false_pos + false_neg + true_neg\n",
        "      confMat = [[true_pos,false_pos],[false_neg,true_neg]]\n",
        "      metrics_dict = {  'accuracy' : (true_pos + true_neg)/total,\n",
        "                        'recall' : true_pos/(true_pos + false_pos),\n",
        "                        'precision' : true_pos/(true_pos + false_neg),\n",
        "                        'confMat'  :  confMat\n",
        "        \n",
        "      }\n",
        "      return metrics_dict      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFPXtsL4gPdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_classifiers = []\n",
        "random_metrics_dict = []\n",
        "for num in range(5):\n",
        "    test = data[num]\n",
        "    train = []\n",
        "    for x in range(5):\n",
        "        if(num == x):\n",
        "            continue\n",
        "        train.append(data[x])\n",
        "    train = [item for sublist in train for item in sublist]\n",
        "    random_classifiers.append(randomBoostClassifier(NaiveBayesClassifier, 200, 0.1))\n",
        "    random_classifiers[num].train(train)\n",
        "    random_predictions = random_classifiers[num].predict(test)\n",
        "  \n",
        "    actual_values = []\n",
        "    for case in test:\n",
        "      actual_values.append(case.split(',')[-1])    \n",
        "    random_metrics_dict.append(random_classifiers[num].metrics(random_predictions, actual_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVGF7jROVwAD",
        "colab_type": "code",
        "outputId": "ae237e2e-7aa5-4be7-a6fa-09beee6e1bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print(\"Metrics of each Randomized Boosted Naive Bayes classifier in 5 fold validation\")\n",
        "print(\"First row of confusion matrix corresponds to 'republican' label\")\n",
        "i = 1\n",
        "for metric in random_metrics_dict:\n",
        "  print(\"Model \",i,\" \",metric)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics of each Randomized Boosted Naive Bayes classifier in 5 fold validation\n",
            "First row of confusion matrix corresponds to 'republican' label\n",
            "Model  1   {'accuracy': 0.896551724137931, 'recall': 0.9487179487179487, 'precision': 0.8409090909090909, 'confMat': [[37, 2], [7, 41]]}\n",
            "Model  2   {'accuracy': 0.9310344827586207, 'recall': 0.8888888888888888, 'precision': 0.9411764705882353, 'confMat': [[32, 4], [2, 49]]}\n",
            "Model  3   {'accuracy': 0.9770114942528736, 'recall': 1.0, 'precision': 0.9375, 'confMat': [[30, 0], [2, 55]]}\n",
            "Model  4   {'accuracy': 0.9540229885057471, 'recall': 0.90625, 'precision': 0.9666666666666667, 'confMat': [[29, 3], [1, 54]]}\n",
            "Model  5   {'accuracy': 0.9655172413793104, 'recall': 1.0, 'precision': 0.9117647058823529, 'confMat': [[31, 0], [3, 53]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "40b3c000-b16e-4471-da97-71bdf2439549",
        "id": "MLEA_K-t4el4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(\"With randomized boosting (200 boosted Naive Bayes classifier,0.1 sampling ratio), the metrics are: \")\n",
        "acc = 0\n",
        "for lst in random_metrics_dict:\n",
        "    acc += lst['accuracy']\n",
        "acc = acc/5\n",
        "print(\"\\nAccuracy:\", acc)\n",
        "rec = 0\n",
        "for lst in random_metrics_dict:\n",
        "    rec += lst['recall']\n",
        "rec = rec/5\n",
        "print(\"\\nRecall:\", rec)\n",
        "prec = 0\n",
        "for lst in random_metrics_dict:\n",
        "    prec += lst['precision']\n",
        "prec = prec/5\n",
        "print(\"\\nPrecision:\", prec)\n",
        "f_score = 0\n",
        "print(\"\\nF-score:\", (2*prec*rec)/(prec+rec))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With randomized boosting (200 boosted Naive Bayes classifier,0.1 sampling ratio), the metrics are: \n",
            "\n",
            "Accuracy: 0.9448275862068964\n",
            "\n",
            "Recall: 0.9487713675213675\n",
            "\n",
            "Precision: 0.9196033868092692\n",
            "\n",
            "F-score: 0.9339597003845094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQqTSYfhp1Q6",
        "colab_type": "text"
      },
      "source": [
        "# Plain Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhgERboARVlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bagging\n",
        "class BaggedClassifier:\n",
        "  def __init__(self, classifier, numberClassifiers, baggingRatio):\n",
        "    self.numberClassifiers = numberClassifiers\n",
        "    self.baggingRatio = baggingRatio\n",
        "    self.classifiers = [classifier() for _ in range(numberClassifiers)]\n",
        "  \n",
        "  def train(self, train):\n",
        "    train_size = len(train)\n",
        "    data_weights = [1/train_size] * train_size\n",
        "\n",
        "    sample_size = int(self.baggingRatio * train_size)\n",
        "    # Bagging\n",
        "    for i in range(self.numberClassifiers):\n",
        "      random_integers = np.random.randint(train_size, size = sample_size)\n",
        "      sampled_data = []\n",
        "      for ind in random_integers:\n",
        "          sampled_data.append(train[ind])\n",
        "      self.classifiers[i].train(sampled_data)\n",
        "\n",
        "\n",
        "  def predict(self, test):\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(test)):\n",
        "      all_predictions = []\n",
        "      for j in range(self.numberClassifiers):\n",
        "        if self.classifiers[j].predict([test[i]])[0] == 'democrat':\n",
        "          all_predictions.append(1)\n",
        "        else:\n",
        "          all_predictions.append(-1)\n",
        "        \n",
        "      if sum(all_predictions) > 0:\n",
        "        predictions.append('democrat')\n",
        "      else:\n",
        "        predictions.append('republican')\n",
        "      \n",
        "    return predictions\n",
        "\n",
        "  def metrics(self, predictions, actual_values):\n",
        "      true_pos = 0\n",
        "      true_neg = 0\n",
        "      false_pos = 0\n",
        "      false_neg = 0   \n",
        "      for i in range(len(predictions)):\n",
        "        if predictions[i] == 'democrat':\n",
        "          if actual_values[i] =='democrat':\n",
        "            true_neg += 1\n",
        "          else:\n",
        "            false_pos += 1\n",
        "        else:\n",
        "          if actual_values[i] == 'republican':\n",
        "            true_pos += 1\n",
        "          else:\n",
        "            false_neg += 1\n",
        "      total =   true_pos + false_pos + false_neg + true_neg\n",
        "      confMat = [[true_pos,false_pos],[false_neg,true_neg]]\n",
        "      metrics_dict = {  'accuracy' : (true_pos + true_neg)/total,\n",
        "                        'recall' : true_pos/(true_pos + false_pos),\n",
        "                        'precision' : true_pos/(true_pos + false_neg),\n",
        "                        'confMat'  :  confMat\n",
        "        \n",
        "      }\n",
        "      return metrics_dict      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSuZGPhETO5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bagged_classifiers = []\n",
        "bagged_metrics_dict = []\n",
        "for num in range(5):\n",
        "    test = data[num]\n",
        "    train = []\n",
        "    for x in range(5):\n",
        "        if(num == x):\n",
        "            continue\n",
        "        train.append(data[x])\n",
        "    train = [item for sublist in train for item in sublist]\n",
        "    bagged_classifiers.append(BaggedClassifier(NaiveBayesClassifier, 200, 0.1))\n",
        "    bagged_classifiers[num].train(train)\n",
        "    bagged_predictions = bagged_classifiers[num].predict(test)\n",
        "  \n",
        "    actual_values = []\n",
        "    for case in test:\n",
        "      actual_values.append(case.split(',')[-1])    \n",
        "    bagged_metrics_dict.append(bagged_classifiers[num].metrics(bagged_predictions, actual_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFyQKKviTUSy",
        "colab_type": "code",
        "outputId": "0855c29b-0d71-4f73-80cb-55fa850b828c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print(\"Metrics of each Bagged Naive Bayes classifier in 5 fold validation\")\n",
        "print(\"First row of confusion matrix corresponds to 'republican' label\")\n",
        "i = 1\n",
        "for metric in bagged_metrics_dict:\n",
        "  print(\"Model \",i,\" \",metric)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics of each Bagged Naive Bayes classifier in 5 fold validation\n",
            "First row of confusion matrix corresponds to 'republican' label\n",
            "Model  1   {'accuracy': 0.8735632183908046, 'recall': 0.8974358974358975, 'precision': 0.8333333333333334, 'confMat': [[35, 4], [7, 41]]}\n",
            "Model  2   {'accuracy': 0.9310344827586207, 'recall': 0.8888888888888888, 'precision': 0.9411764705882353, 'confMat': [[32, 4], [2, 49]]}\n",
            "Model  3   {'accuracy': 0.9770114942528736, 'recall': 1.0, 'precision': 0.9375, 'confMat': [[30, 0], [2, 55]]}\n",
            "Model  4   {'accuracy': 0.9540229885057471, 'recall': 0.90625, 'precision': 0.9666666666666667, 'confMat': [[29, 3], [1, 54]]}\n",
            "Model  5   {'accuracy': 0.9540229885057471, 'recall': 0.9354838709677419, 'precision': 0.9354838709677419, 'confMat': [[29, 2], [2, 54]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSAFep0rTboo",
        "colab_type": "code",
        "outputId": "67043700-56e1-4416-9691-fd985ae03a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(\"With bagging (200 Naive Bayes classifiers, 0.1 bagging ratio), the metrics are: \")\n",
        "acc = 0\n",
        "for lst in bagged_metrics_dict:\n",
        "    acc += lst['accuracy']\n",
        "acc = acc/5\n",
        "print(\"\\nAccuracy:\", acc)\n",
        "rec = 0\n",
        "for lst in bagged_metrics_dict:\n",
        "    rec += lst['recall']\n",
        "rec = rec/5\n",
        "print(\"\\nRecall:\", rec)\n",
        "prec = 0\n",
        "for lst in bagged_metrics_dict:\n",
        "    prec += lst['precision']\n",
        "prec = prec/5\n",
        "print(\"\\nPrecision:\", prec)\n",
        "f_score = 0\n",
        "print(\"\\nF-score:\", (2*prec*rec)/(prec+rec))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With bagging (200 Naive Bayes classifiers, 0.1 bagging ratio), the metrics are: \n",
            "\n",
            "Accuracy: 0.9379310344827587\n",
            "\n",
            "Recall: 0.9256117314585056\n",
            "\n",
            "Precision: 0.9228320683111955\n",
            "\n",
            "F-score: 0.9242198098761596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65v0Lp7Pp9Yi",
        "colab_type": "text"
      },
      "source": [
        "#Plain Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzhNSl3WVHrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Boosting\n",
        "class PlainBoosting:\n",
        "  def __init__(self, classifier, numberBoosting, samplingRatio):\n",
        "    self.numberBoosting = numberBoosting\n",
        "    self.samplingRatio = samplingRatio\n",
        "    self.classifiers = [classifier() for _ in range(numberBoosting)]\n",
        "    self.alphas = [0]*numberBoosting\n",
        "  \n",
        "  def train(self, train):\n",
        "    train_size = len(train)\n",
        "    data_weights = [1/train_size] * train_size\n",
        "    target_value =[]\n",
        "    for case in train:\n",
        "      target_value.append(case.split(',')[-1])   \n",
        "    sample_size = int(self.samplingRatio * train_size)\n",
        "\n",
        "    indices = list(range(0,len(train)))\n",
        "    # Bagging\n",
        "    for i in range(self.numberBoosting):\n",
        "      sampled_data = {}\n",
        "      for ind in indices:\n",
        "        sampled_data[ind] = train[ind]\n",
        "        self.classifiers[i].train(list(sampled_data.values()))\n",
        "\n",
        "      # Boosting\n",
        "      error = 0\n",
        "      for j in range(len(train)):\n",
        "        if j in indices:\n",
        "          predicted_value = self.classifiers[i].predict([sampled_data[j]])[0]\n",
        "          if predicted_value != target_value[j]:\n",
        "            error += data_weights[j]\n",
        "\n",
        "      # Normalising error\n",
        "      error = error / sum(data_weights)\n",
        "\n",
        "      # Setting alpha\n",
        "      if error!=0:\n",
        "        self.alphas[i] = (1/2)*math.log((1-error)/error)\n",
        "      else: \n",
        "        self.alphas[i] = 349\n",
        "\n",
        "      # Updating weights\n",
        "      for j in range(len(train)):\n",
        "        if j in indices:\n",
        "          predicted_value = self.classifiers[i].predict([sampled_data[j]])[0]\n",
        "          if predicted_value != target_value[j]:\n",
        "              data_weights[j]  = data_weights[j]*math.exp((self.alphas[i]))\n",
        "          else:\n",
        "              data_weights[j]  = data_weights[j]*math.exp((self.alphas[i]))\n",
        "      \n",
        "      sum_data_weights = sum(data_weights)\n",
        "      for j in range(len(train)):\n",
        "         data_weights[j] = data_weights[j]/ sum_data_weights\n",
        "      \n",
        "      data_weights_threshold = sorted(data_weights, reverse=True)[int(len(data_weights)*0.5)]\n",
        "\n",
        "      indices = []\n",
        "      for ind in range(0,len(data_weights)):\n",
        "        if data_weights[ind] >= data_weights_threshold:\n",
        "          indices.append(ind)\n",
        "      random.shuffle(indices)     \n",
        "      indices = indices[:int(len(indices)*self.samplingRatio)]\n",
        "\n",
        "  def predict(self, test):\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(test)):\n",
        "      all_predictions = []\n",
        "      for j in range(self.numberBoosting):\n",
        "        if self.classifiers[j].predict([test[i]])[0] == 'democrat':\n",
        "          all_predictions.append(self.alphas[j])\n",
        "        else:\n",
        "          all_predictions.append(-1* self.alphas[j])\n",
        "        \n",
        "      if sum(all_predictions) > 0:\n",
        "        predictions.append('democrat')\n",
        "      else:\n",
        "        predictions.append('republican')\n",
        "      \n",
        "    return predictions\n",
        "\n",
        "  def metrics(self, predictions, actual_values):\n",
        "      true_pos = 0\n",
        "      true_neg = 0\n",
        "      false_pos = 0\n",
        "      false_neg = 0   \n",
        "      for i in range(len(predictions)):\n",
        "        if predictions[i] == 'democrat':\n",
        "          if actual_values[i] =='democrat':\n",
        "            true_neg += 1\n",
        "          else:\n",
        "            false_pos += 1\n",
        "        else:\n",
        "          if actual_values[i] == 'republican':\n",
        "            true_pos += 1\n",
        "          else:\n",
        "            false_neg += 1\n",
        "      total =   true_pos + false_pos + false_neg + true_neg\n",
        "      confMat = [[true_pos,false_pos],[false_neg,true_neg]]\n",
        "      metrics_dict = {  'accuracy' : (true_pos + true_neg)/total,\n",
        "                        'recall' : true_pos/(true_pos + false_pos),\n",
        "                        'precision' : true_pos/(true_pos + false_neg),\n",
        "                        'confMat'  :  confMat\n",
        "        \n",
        "      }\n",
        "      return metrics_dict      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJRsg0ATY4WG",
        "colab_type": "code",
        "outputId": "372d2209-c71c-433e-8a74-5accbe79533f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "PlainBoosting_classifiers = []\n",
        "PlainBoosting_metrics_dict = []\n",
        "for num in tqdm(range(3)):\n",
        "    test = data[num]\n",
        "    train = []\n",
        "    for x in range(5):\n",
        "        if(num == x):\n",
        "            continue\n",
        "        train.append(data[x])\n",
        "    train = [item for sublist in train for item in sublist]\n",
        "    random.shuffle(train)\n",
        "    PlainBoosting_classifiers.append(PlainBoosting(NaiveBayesClassifier, 200, 0.6))\n",
        "    PlainBoosting_classifiers[num].train(train)\n",
        "    PlainBoosting_predictions = PlainBoosting_classifiers[num].predict(test)\n",
        "  \n",
        "    actual_values = []\n",
        "    for case in test:\n",
        "      actual_values.append(case.split(',')[-1])    \n",
        "    PlainBoosting_metrics_dict.append(PlainBoosting_classifiers[num].metrics(PlainBoosting_predictions, actual_values))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|███▎      | 1/3 [00:13<00:27, 13.65s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [00:25<00:13, 13.25s/it]\u001b[A\n",
            "100%|██████████| 3/3 [00:33<00:00, 11.67s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvuE-_uc5no",
        "colab_type": "code",
        "outputId": "273a7fd0-eacb-4a26-a6f7-185a23304eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(\"Metrics of each Boosted Naive Bayes classifier in 3 fold validation\")\n",
        "print(\"First row of confusion matrix corresponds to 'republican' label\")\n",
        "i = 1\n",
        "for metric in PlainBoosting_metrics_dict:\n",
        "  print(\"Model \",i,\" \",metric)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics of each Boosted Naive Bayes classifier in 3 fold validation\n",
            "First row of confusion matrix corresponds to 'republican' label\n",
            "Model  1   {'accuracy': 0.8620689655172413, 'recall': 0.8974358974358975, 'precision': 0.813953488372093, 'confMat': [[35, 4], [8, 40]]}\n",
            "Model  2   {'accuracy': 0.8620689655172413, 'recall': 0.9166666666666666, 'precision': 0.7857142857142857, 'confMat': [[33, 3], [9, 42]]}\n",
            "Model  3   {'accuracy': 0.9425287356321839, 'recall': 1.0, 'precision': 0.8571428571428571, 'confMat': [[30, 0], [5, 52]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOFtkGDYgNEh",
        "colab_type": "code",
        "outputId": "66a50fe3-0cd9-4b44-e909-386b7ddbfa57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(\"With plain boosting (200 Naive Bayes classifiers), the metrics are: \")\n",
        "acc = 0\n",
        "for lst in PlainBoosting_metrics_dict:\n",
        "    acc += lst['accuracy']\n",
        "acc = acc/3\n",
        "print(\"\\nAccuracy:\", acc)\n",
        "rec = 0\n",
        "for lst in PlainBoosting_metrics_dict:\n",
        "    rec += lst['recall']\n",
        "rec = rec/3\n",
        "print(\"\\nRecall:\", rec)\n",
        "prec = 0\n",
        "for lst in PlainBoosting_metrics_dict:\n",
        "    prec += lst['precision']\n",
        "prec = prec/3\n",
        "print(\"\\nPrecision:\", prec)\n",
        "f_score = 0\n",
        "print(\"\\nF-score:\", (2*prec*rec)/(prec+rec))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With plain boosting (200 Naive Bayes classifiers), the metrics are: \n",
            "\n",
            "Accuracy: 0.8888888888888888\n",
            "\n",
            "Recall: 0.9380341880341879\n",
            "\n",
            "Precision: 0.8189368770764119\n",
            "\n",
            "F-score: 0.8744489921252843\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}